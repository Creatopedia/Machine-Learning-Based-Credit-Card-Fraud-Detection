# -*- coding: utf-8 -*-
"""2023Dataset_Sup|Unsuper|Ensembled_Models

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JyTUEsklAgCvcUw_W3qFYjxoTlMvmFLO
"""

# -*- coding: utf-8 -*-
"""Super|Unsuper.ipynb and Ensembled Model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QCPWks_ISEtvbJASWeTXEZX2mHj7J_KB
"""
from google.colab import drive
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from collections import Counter
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from xgboost import XGBClassifier, plot_importance
from sklearn.cluster import KMeans
from sklearn.metrics import (
    roc_auc_score, roc_curve, confusion_matrix,
    recall_score, classification_report,
    accuracy_score, f1_score
)
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt
import seaborn as sns

# Additional imports for the second set of code
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.ensemble import IsolationForest
from sklearn.tree import DecisionTreeClassifier

# Mount Google Drive
drive.mount('/content/drive')

# Load the data
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/creditcard_2023.csv")
data.head()

# Data preprocessing
data.isnull().sum()

# Explore data
data.info()
data.shape
data.describe()

# Correlation heatmap
dataplot = sns.heatmap(data.corr(), cmap="YlGnBu")
plt.show()

# Class distribution count plot
plt.figure(figsize=(5, 5))
sns.countplot(x="Class", data=data)
(data["Class"].value_counts() / len(data)) * 100

# Display the percentage distribution as text
class_distribution_percent = (data["Class"].value_counts() / len(data)) * 100
for index, value in class_distribution_percent.items():
    plt.text(index, value, f'{value:.2f}%', ha='center', va='bottom')

# Show the plot
plt.title('Class Distribution and Percentage')
plt.show()

# Prepare data for modeling
X = data.drop("Class", axis=1)
y = data["Class"]


# Normalize features
scale = MinMaxScaler()
X = scale.fit_transform(X)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
X_train.shape, y_train.shape

# Supervised Model 1: Logistic Regression
lr = LogisticRegression()
lr.fit(X_train, y_train)

# Evaluation and visualization
y_pred_lr = lr.predict(X_test)
print("Logistic Regression Classification Report:\n", classification_report(y_test, y_pred_lr))

# ROC Curve for Logistic Regression
y_pred_proba_lr = lr.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_pred_proba_lr)
auc = roc_auc_score(y_test, y_pred_proba_lr)
plt.plot(fpr, tpr, label="Logistic Regression, auc="+str(auc))
plt.legend(loc=4)
plt.show()

# Confusion Matrix for Logistic Regression
cf_matrix_lr = confusion_matrix(y_test, y_pred_lr)
print("Confusion Matrix (Logistic Regression):\n", cf_matrix_lr)

# Save Logistic Regression model
pickle.dump(lr, open('lr_model (auc = 0.99).pkl', 'wb'))

# Supervised Model 2: XGBoost
xgb = XGBClassifier()
xgb.fit(X_train, y_train)

# Evaluation and visualization
y_pred_xgb = xgb.predict(X_test)
print("XGBoost Classification Report:\n", classification_report(y_test, y_pred_xgb))

# ROC Curve for XGBoost
y_pred_proba_xgb = xgb.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_pred_proba_xgb)
auc = roc_auc_score(y_test, y_pred_proba_xgb)
plt.plot(fpr, tpr, label="XGBoost, auc="+str(auc))
plt.legend(loc=4)
plt.show()

# Confusion Matrix for XGBoost
cf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)
print("Confusion Matrix (XGBoost):\n", cf_matrix_xgb)

# Save XGBoost model
pickle.dump(xgb, open('xgb_model (auc = 0.99).pkl', 'wb'))

# Plot Feature Importances for XGBoost
plot_importance(xgb)
plt.show()

# Supervised Model 3: Random Forest
rf = RandomForestClassifier()
rf.fit(X_train, y_train)

# Evaluation and visualization
y_pred_rf = rf.predict(X_test)
print("Random Forest Classification Report:\n", classification_report(y_test, y_pred_rf))

# ROC Curve for Random Forest
y_pred_proba_rf = rf.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_pred_proba_rf)
auc = roc_auc_score(y_test, y_pred_proba_rf)
plt.plot(fpr, tpr, label="Random Forest, auc="+str(auc))
plt.legend(loc=4)
plt.show()

# Confusion Matrix for Random Forest
cf_matrix_rf = confusion_matrix(y_test, y_pred_rf)
print("Confusion Matrix (Random Forest):\n", cf_matrix_rf)

# Save Random Forest model
pickle.dump(rf, open('rf_model (auc = 0.99).pkl', 'wb'))

# Unsupervised Model 1: KMeans Clustering
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(X_train)

# Predictions and Evaluation
y_pred_kmeans = kmeans.predict(X_test)
print("KMeans Clustering Classification Report:\n", classification_report(y_test, y_pred_kmeans))


# Anomaly Detection for KMeans
distance_to_center_kmeans = np.min(kmeans.transform(X_test), axis=1)
anomaly_rate_kmeans = np.mean(y_pred_kmeans)

# Print and visualize anomalies for KMeans
print("KMeans Clustering Anomaly Detection Report:")
print("Distance to Center:\n", distance_to_center_kmeans)
print("Anomaly Rate:", anomaly_rate_kmeans)

# Visualization of KMeans Clustering Anomaly Detection
plt.figure(figsize=(10, 8))
plt.scatter(X_test[:, 0], X_test[:, 1], c=distance_to_center_kmeans, cmap='viridis', s=50, alpha=0.5)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', s=200, marker='X')
plt.title('KMeans Clustering Anomaly Detection')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Distance to Center')
plt.show()

# Unsupervised Model 2: Isolation Forest
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
isolation_forest = IsolationForest(contamination=0.01, random_state=42)
isolation_forest.fit(X_train)

# Predictions and Evaluation
y_pred_iso_forest = isolation_forest.predict(X_test)
y_pred_iso_forest[y_pred_iso_forest == 1] = 0
y_pred_iso_forest[y_pred_iso_forest == -1] = 1

print("Isolation Forest Classification Report:\n", classification_report(y_test, y_pred_iso_forest))

# Confusion Matrix for Isolation Forest
cf_matrix_iso_forest = confusion_matrix(y_test, y_pred_iso_forest)
print("Confusion Matrix (Isolation Forest):\n", cf_matrix_iso_forest)

# Anomaly Detection for Isolation Forest
anomaly_scores_iso_forest = isolation_forest.decision_function(X_test)
anomaly_rate_iso_forest = np.mean(y_pred_iso_forest)

# Print and visualize anomalies for Isolation Forest
print("Isolation Forest Anomaly Detection Report:")
print("Anomaly Scores:\n", anomaly_scores_iso_forest)
print("Anomaly Rate:", anomaly_rate_iso_forest)

# Visualization of Isolation Forest Anomaly Detection
plt.figure(figsize=(10, 8))
plt.scatter(X_test[:, 0], X_test[:, 1], c=anomaly_scores_iso_forest, cmap='viridis', s=50, alpha=0.5)
plt.title('Isolation Forest Anomaly Detection')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.colorbar(label='Anomaly Score')
plt.show()

# Custom Classifier for IsolationForest
class IsolationForestClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, contamination=0.01, random_state=None):
        self.contamination = contamination
        self.random_state = random_state
        self.isolation_forest = IsolationForest(contamination=self.contamination, random_state=self.random_state)

    def fit(self, X, y=None):
        self.isolation_forest.fit(X)
        return self

    def predict(self, X):
        # Convert IsolationForest output to binary predictions
        return np.where(self.isolation_forest.predict(X) == -1, 1, 0)

    def decision_function(self, X):
        # Return anomaly score as a decision function
        return -self.isolation_forest.decision_function(X)

    def predict_proba(self, X):
        # Simulate probabilities based on anomaly scores
        decision_function = self.decision_function(X)
        min_decision = np.min(decision_function)
        max_decision = np.max(decision_function)
        normalized_decision = (decision_function - min_decision) / (max_decision - min_decision)
        # Probability is the complement of the anomaly score
        probabilities = 1 - normalized_decision
        return np.column_stack([1 - probabilities, probabilities])

# Supervised Model 1: Logistic Regression
lr = LogisticRegression()
lr.fit(X_train, y_train)

# Supervised Model 2: XGBoost
xgb = XGBClassifier()
xgb.fit(X_train, y_train)

# Supervised Model 3: RandomForest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

# Unsupervised Model 1: KMeans Clustering
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans_pipeline = Pipeline([
    ("kmeans", kmeans),
    ("classifier", DecisionTreeClassifier())  # Using a classifier, e.g., DecisionTreeClassifier
])

# Unsupervised Model 2: Isolation Forest
isolation_forest_classifier = IsolationForestClassifier(contamination=0.01, random_state=42)
# Hybrid Model: Ensemble of Logistic Regression, XGBoost, Random Forest, KMeans, Isolation Forest
ensemble_model = VotingClassifier(estimators=[
    ('logistic_regression', lr),
    ('xgboost', xgb),
    ('kmeans', kmeans_pipeline),
     ('random_forest', rf),
    ('isolation_forest', isolation_forest_classifier)
], voting='soft')

ensemble_model.fit(X_train, y_train)

# Predictions for the Ensemble Model
y_pred_ensemble = ensemble_model.predict(X_test)

# Classification Report for Ensemble Model
classification_rep = classification_report(y_test, y_pred_ensemble)
print("Classification Report (Ensemble Model):\n", classification_rep)

# Calculate Accuracy, F1-score, and Recall
accuracy = accuracy_score(y_test, y_pred_ensemble)
f1 = f1_score(y_test, y_pred_ensemble)
recall = recall_score(y_test, y_pred_ensemble)

# Print Accuracy, F1-score, and Recall
print("Accuracy (Ensemble Model):", accuracy)
print("F1-score (Ensemble Model):", f1)
print("Recall (Ensemble Model):", recall)

# ROC Curve for Ensemble Model
y_pred_proba_ensemble = ensemble_model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_pred_proba_ensemble)
auc = roc_auc_score(y_test, y_pred_proba_ensemble)
plt.plot(fpr, tpr, label="Ensemble Model, auc="+str(auc))
plt.legend(loc=4)
plt.show()

# Confusion Matrix for Ensemble Model
cf_matrix_ensemble = confusion_matrix(y_test, y_pred_ensemble)
print("Confusion Matrix (Ensemble Model):\n", cf_matrix_ensemble)

# Save Ensemble Model
pickle.dump(ensemble_model, open('ensemble_model.pkl', 'wb'))

# Confusion Matrix for Ensemble Model
cf_matrix_ensemble = confusion_matrix(y_test, y_pred_ensemble)
print("Confusion Matrix (Ensemble Model):\n", cf_matrix_ensemble)

# Plot Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cf_matrix_ensemble, annot=True, fmt='g', cmap='Blues', cbar=False)
plt.title("Confusion Matrix - Ensemble Model")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# Bar Plot for Fraud and Non-Fraud Counts
plt.figure(figsize=(8, 6))
sns.countplot(x=y_test, hue=y_pred_ensemble)
plt.title("Fraud and Non-Fraud Counts - Ensemble Model")
plt.xlabel("Actual Class")
plt.ylabel("Count")
plt.legend(title="Predicted Class", loc="upper right", labels=["Non-Fraud", "Fraud"])
plt.show()

# Assuming 'X_test' is your test data

# Predictions for the Ensemble Model
y_pred_ensemble = ensemble_model.predict(X_test)

# Anomaly Detection Scores
anomaly_scores = ensemble_model.predict_proba(X_test)[:, 1]

# Anomaly Detection Rates
anomaly_rate = np.mean(y_pred_ensemble)

# Print and visualize anomalies
print("Ensemble Model Classification Report:\n", classification_report(y_test, y_pred_ensemble))

# Visualize anomalies
plt.figure(figsize=(10, 8))
plt.scatter(X_test[y_pred_ensemble == 0][:, 0], X_test[y_pred_ensemble == 0][:, 1], label='Non-Anomalies', c='green', s=10)
plt.scatter(X_test[y_pred_ensemble == 1][:, 0], X_test[y_pred_ensemble == 1][:, 1], label='Anomalies', c='red', s=30, marker='x')
plt.title('Ensemble Model Anomaly Detection')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.show()

# Print Anomaly Scores and Rate
print("Anomaly Scores:\n", anomaly_scores)
print("Anomaly Rate:", anomaly_rate)